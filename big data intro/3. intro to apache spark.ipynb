{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245861ff",
   "metadata": {},
   "source": [
    "# Spark intro:\n",
    "- Unified engine for large scale data analytics.\n",
    "- Its a distributed data processing framework.\n",
    "\n",
    "## Data Lake flow architecture\n",
    "- We collect data from various sources and store that, initially in RDBMS.\n",
    "- Later as discussed due to 3V of big data challenge we started using cluster computing.\n",
    "- In the onset, we stored all the structured, semi structured, and unstructured data into data lake (initially HDFS now cloud)\n",
    "- We peformed tranformation  (initially using Hadoop map reduce and now Spark) and saved it in the lake itself \n",
    "- from there we used the data from reporting and ML \n",
    "- had 2 challenges that it was slow for reporting purposes and didnt have consistency\n",
    "- So we introduced DW in between, where\n",
    "    - we stored all data to lake\n",
    "    - transformed using spark\n",
    "    - for BI and reporting we add the finished sets to DW\n",
    "    - for ML and AI they still load from lake or DW \n",
    "\n",
    "## Modern day lake components\n",
    "- Data collection and ingestion: procuring data from multiple sources (eg tools kafka, informatica, talend)\n",
    "- Data storage and Management: Onpremise Hadoop or cloud servers like S3, Azure Blob, Google cloud storage\n",
    "- Data processing and trasformation: all computation happens like quality check, transformation, aggreagation adn extracting business logic (Apache spark)\n",
    "- Data Access and Retrieval: Usage for the real life business logics (applications, dashboards, APIs)\n",
    "\n",
    "## Spark ecosystem:\n",
    "- layers \n",
    "    - spark core\n",
    "    - DSL/library and APIs\n",
    "\n",
    "### core layer:\n",
    "- Core apis in python, java, scala, r\n",
    "- Spark computing engine\n",
    "- Cluster management\n",
    "- storage system\n",
    "- whole runs on cluster to give parallel computings.\n",
    "- doesnt own the cluster management and storage management like hadoop so we need another cluster management tool. (YARN, Mesos, kubernetes) and storage manageement (S3, GCS, Azure blob, HDFS, CFS)\n",
    "- Its only \"RUN/MANAGE DATA PROCESSING WORKLOAD\" owned by spark engine here rest all can be done with different tools.\n",
    "\n",
    "    #### core APIs:\n",
    "    - programming interface offering programing in 4 main language\n",
    "    - to write data processing logic in initial days\n",
    "    - based on RDDs which is hard to learn lack performance optimizations. \n",
    "    - Offers highest flexibilities\n",
    "\n",
    "    #### DSL/Libraries:\n",
    "    - set of libraries and DSL \n",
    "    - developed by spark community over core api for ease and based on core apis \n",
    "    - widely used.\n",
    "    - So the working is done on the top layer (dsl/libraries)\n",
    "    - this is based on the core apis\n",
    "    - the core apis in turn use the spark engine for computations in colab with RM, Cluster, Storage management\n",
    "    Components (avl in 4 languages as discussed)\n",
    "        - Spark SQL & Spark Data frames (allows sql to process and dataframes)\n",
    "        - streaming (streaming data problems)\n",
    "        - Mllib (for machine learning)\n",
    "        - GraphX (for graph proessing)\n",
    "\n",
    "## Benefits of spark:\n",
    "- abstraction: with its libs its easy to use and hides the underlying complexities of code and cluster computing. Its almost similar like that of working with single programming with python or java\n",
    "- unified platform: Sql, ML, Storage management all in fav language\n",
    "- ease of use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298cb60",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
