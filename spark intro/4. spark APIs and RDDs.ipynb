{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bf3e61",
   "metadata": {},
   "source": [
    "# RDD\n",
    "- RDD is the underlying data structure beneath both spark tables and dataframe.\n",
    "- Using the APIs whatever transformation we do they convert into RDDs and then processes using spark engine. \n",
    "- Stands for resilient distributed datset.\n",
    "- Is a dataset, data structure to hold data similar to dataframe.\n",
    "- Unlike dataframe these are language naitve object and dont have row column structure and schema.\n",
    "- Rdds are broken internally as partitions for distributed process.\n",
    "- They are resilient as they are fault tolerant as they store data about how it was created.\n",
    "    example while loading a partition if one core fails then driver allocates the assigned partitions to other core and they create the partition as the partition has the details on how to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2917a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders(order_id='1007', order_date='01/18/2023', item_type='Clothes', unit_price=132.45, unit_cost=90.1, total_revenue=13245.0, total_cost=9010.0, total_profit=4235.0)\n",
      "orders(order_id='1012', order_date='01/23/2023', item_type='Clothes', unit_price=145.9, unit_cost=112.0, total_revenue=14590.0, total_cost=11200.0, total_profit=3390.0)\n",
      "orders(order_id='1017', order_date='01/28/2023', item_type='Clothes', unit_price=110.25, unit_cost=85.0, total_revenue=11025.0, total_cost=8500.0, total_profit=2525.0)\n",
      "orders(order_id='1022', order_date='02/02/2023', item_type='Clothes', unit_price=128.4, unit_cost=101.3, total_revenue=12840.0, total_cost=10130.0, total_profit=2710.0)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from webbrowser import get\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    conf= SparkConf().setAppName(\"RDD\").setMaster(\"local[*]\")\n",
    "    \n",
    "    # dataframe apis are based on spark session whereas RDD apis are based on spark context\n",
    "    # description of what spark context is:\n",
    "    # SparkContext is the entry point to Spark functionality. \n",
    "    # It is the main entry point for Spark functionality and is responsible for coordinating the execution of tasks across the cluster.\n",
    "    # It allows the Spark application to access the cluster, create RDDs, and perform operations on them.\n",
    "    \n",
    "    # 2 methods to get spark context\n",
    "\n",
    "    #sc = SparkContext(conf=conf) # directly creating a spark context object\n",
    "    \n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('Hello RDD')\n",
    "             .master('local[*]')\n",
    "             .getOrCreate())\n",
    "    sc2= spark.sparkContext \n",
    "    # creating spark context using spark session\n",
    "    # Spark session is higher level object created on top of context for improvement and still holds context and uses internally\n",
    "    \n",
    "\n",
    "    # creating an RDD from a list of tuples\n",
    "    linesrdd = sc2.textFile (r\"C:\\Users\\shubh\\OneDrive\\Desktop\\orders.txt\")\n",
    "    # each record in RDD is line of text from from.\n",
    "    # spark context helps read text, binary, sequence, hadoop and object file it means that are raw and fundamental.\n",
    "    # doesn't let you work with files like csv, excel etc.\n",
    "    # it is a low level API and gives you more control over the data and how it is processed.\n",
    "\n",
    "\n",
    "    # How to process RDD.\n",
    "    # RDDs offered only basic transformation like map, reduce, filter.\n",
    "    # most of the transformations were done by lambda functions taking custom code leaving the user to do the heavy lifting.\n",
    "\n",
    "    partitionedrdd= linesrdd.repartition(2)\n",
    "    structuredrdd = partitionedrdd.map(lambda line : line.replace('\"','').split(',')) # since the data file is loaded as line of text in txt file we need to restructure the data\n",
    "    # with this the input was line of text and the result is list of text\n",
    "    # the txt files should not have header in it as if there then transforamtion breaks\n",
    "    # if header is present remove the first row\n",
    "\n",
    "        # # Extract header\n",
    "        # header = rawRDD.first()\n",
    "\n",
    "        # # Remove the header\n",
    "        # dataRDD = rawRDD.filter(lambda line: line != header)\n",
    "\n",
    "    orders = namedtuple('orders',[\"order_id\",\"order_date\",\"item_type\",\"unit_price\",\"unit_cost\",\"total_revenue\",\"total_cost\",\"total_profit\"])\n",
    "    # this tuple helps create a schema for the data and gives a name to each column in the data.\n",
    "\n",
    "    # couple of transformations:\n",
    "    selectRDD = structuredrdd.map(lambda x: orders(x[0],x[1],x[2],float(x[3]),float(x[4]),float(x[5]),float(x[6]),float(x[7])))\n",
    "    filteredRDD = selectRDD.filter(lambda x: x.item_type == 'Clothes')\n",
    "    \n",
    "    for i in filteredRDD.take(10):\n",
    "        print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
