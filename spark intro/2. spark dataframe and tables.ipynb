{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c2d77a",
   "metadata": {},
   "source": [
    "## Spark Storage:\n",
    "Spark stores data in 2 ways \n",
    "- Spark table\n",
    "- Spark dataframe\n",
    "\n",
    "A Table has 3 components \n",
    "- metadata\n",
    "- storage layer \n",
    "- coputing layer (sql)\n",
    "When we query something SQL first check in metadata if the schema information is correct else throws error. Then it takes the same data from the storage and return\n",
    "\n",
    "Spark also works the same way\n",
    "\n",
    "## Spark Dataframe:\n",
    "- Spark dataframe is a distributed collection of data organised in rows and columns.\n",
    "- its a temporary object thats runs during spark session and then terminates.\n",
    "- It is immutable, we cannot change structure. We need to define new variable and save the mutted dataframe to that.\n",
    "- Spark dataframe is similar to that of a table only difference is it doesnt have a metadata rather a catalog which stores the metadata info only during the runtime and terminates once spark app terminates\n",
    "- It is an in memory object. IT runs when the application runs and terminates post closure thus its temporary object\n",
    "- Dataframe cannot be empty as it initially loads data then defines schema unlike table where we degine schema then load data. \n",
    "- Dataframe offers apis and not sql expressions.\n",
    "\n",
    "## Spark tables:\n",
    "- Spark table is a distributed collection of data stored in rows and columns which is a persistent database object.\n",
    "- It enables schema enforcement.\n",
    "- Can be queried using SQL.\n",
    "- It stores Metadata, storage files and computing using sql and not apis.\n",
    "\n",
    "    ### Types:\n",
    "    - Temp view:\n",
    "    - Global temp view\n",
    "    - Maanged tables\n",
    "    - External table\n",
    "    - Delta table\n",
    "\n",
    "    #### Temp view: \n",
    "    availabe only for the current session\n",
    "    not saved any form of data metadata or actual data\n",
    "    #### Global temp view:\n",
    "    available across multiple spark sessions\n",
    "    stored in global_temp database\n",
    "    ends only when app stops\n",
    "    #### managed table:\n",
    "    works like a proper table\n",
    "    #### external table:\n",
    "    spark only manages the metadata and gets the data from other sources like s3 etc.\n",
    "\n",
    "| Type               | Data Persisted | Session Scope     | ACID | Path Control     |\n",
    "|--------------------|----------------|-------------------|------|------------------|\n",
    "| Temp Table         | No             | Current only      | ❌   | No               |\n",
    "| Global Temp Table  | No             | All sessions      | ❌   | No               |\n",
    "| Managed Table      | Yes            | Permanent         | ❌   | Spark-managed    |\n",
    "| External Table     | Yes            | Permanent         | ❌   | User-managed     |\n",
    "| Delta Table        | Yes            | Permanent         | ✅   | Flexible         |\n",
    "\n",
    "spark dataframe and tables are interchangeable. You can convert table to dataframe and dataframe to table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e47a25",
   "metadata": {},
   "source": [
    "Typical data processing is 3 steps :\n",
    "- Read data\n",
    "- Process data\n",
    "- Write the data\n",
    "\n",
    "Dataframe details:\n",
    "- Dataframe is 2D table like structure inspired by pandas df\n",
    "- They are distributed table with named columna nd well defined schema \n",
    "- Dataframe must have a column name and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0965443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------------+------------------------------+----------------+----------+----+----------+----------+------+----+\n",
      "|booking_id|doj       |day    |booking_date|booking_route                 |route_type      |seat_count|fare|ry_user_id|phone_no  |gender|age |\n",
      "+----------+----------+-------+------------+------------------------------+----------------+----------+----+----------+----------+------+----+\n",
      "|4231262   |12/12/2023|Tuesday|12/13/2023  |Vellore-Bangalore (Bengaluru) |sub/sector_route|1         |592 |73006237  |9514576721|M     |35  |\n",
      "|4231263   |12/12/2023|Tuesday|12/13/2023  |Delhi-Lucknow                 |primary         |1         |499 |51671687  |7908882499|M     |NULL|\n",
      "|4231264   |12/12/2023|Tuesday|12/13/2023  |Hyderabad-Guntur              |primary         |1         |522 |74522222  |9398969525|F     |33  |\n",
      "|4231272   |12/12/2023|Tuesday|12/13/2023  |Tirupati-Bangalore (Bengaluru)|sub/sector_route|1         |499 |48241626  |9985998594|M     |34  |\n",
      "|4231280   |12/12/2023|Tuesday|12/13/2023  |Trichy-Chennai                |sub/sector_route|1         |592 |38520311  |9790479005|M     |28  |\n",
      "+----------+----------+-------+------------+------------------------------+----------------+----------+----+----------+----------+------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- booking_id: integer (nullable = true)\n",
      " |-- doj: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- booking_date: string (nullable = true)\n",
      " |-- booking_route: string (nullable = true)\n",
      " |-- route_type: string (nullable = true)\n",
      " |-- seat_count: integer (nullable = true)\n",
      " |-- fare: integer (nullable = true)\n",
      " |-- ry_user_id: integer (nullable = true)\n",
      " |-- phone_no: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data frame example:\n",
    "\n",
    "# Read the data \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"Read Data\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Read the data\n",
    "    spark_df = spark.read.csv(\n",
    "        r\"C:\\Users\\shubh\\OneDrive\\Desktop\\validating data.csv\", \n",
    "        header=True, #check the first row to infer column row\n",
    "        inferSchema=True # make intelligent guess of the data type of each column\n",
    "        )\n",
    "    \n",
    "    spark_df.show(5, False)\n",
    "    spark_df.printSchema()\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
