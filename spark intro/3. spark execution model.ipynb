{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679e2c4b",
   "metadata": {},
   "source": [
    "# DataFrame partition and distribution process\n",
    "- In distributed storage systems (like HDFS, S3, etc.), data is stored as blocks across multiple nodes, which are naturally partitioned.\n",
    "\n",
    "- When we read data into Spark, it's loaded into memory as partitions — think of each partition as a smaller, manageable chunk of the overall DataFrame that can be processed independently and in parallel.\n",
    "\n",
    "- The Driver program (your main Spark application entry point) sends a read command, and based on metadata from the cluster manager and storage system, it constructs a logical plan for the DataFrame and determines how it will be partitioned.\n",
    "\n",
    "- At this stage (before any action), the DataFrame is lazily evaluated — Spark only builds the execution plan (DAG) and knows what needs to be done, but nothing is executed yet.\n",
    "\n",
    "- When you trigger an action (e.g., show(), collect(), write(), etc.), the Driver requests resources (containers/executors) from the cluster manager (like YARN or Spark Standalone).\n",
    "\n",
    "- The cluster manager launches executors on worker nodes, and each executor is assigned one or more partitions of the DataFrame to process.\n",
    "\n",
    "- The Driver coordinates the task execution across nodes and eventually collects the results (if required).\n",
    "\n",
    "- You can control the number of partitions using .repartition() or .coalesce() for better performance tuning.\n",
    "\n",
    "- The number of partitions affects parallelism: more partitions = better parallelism (to an extent), but also more overhead.\n",
    "\n",
    "# Spark Operations:\n",
    "\n",
    "## Transformations\n",
    "- Transformations are operations that create a new DataFrame from an existing one without modifying the original. They are lazy, meaning they don't trigger execution until an action is called.\n",
    "- There are two types of transformations:\n",
    "    - Narrow Dependency Transformations\n",
    "        - Each output partition depends on only one input partition.\n",
    "        - Transformations can be performed independently on each partition.\n",
    "        - These are faster and do not require data shuffling.\n",
    "        - Examples: filter, select, where, union\n",
    "\n",
    "    - Wide Dependency Transformations\n",
    "        - Each output partition may depend on multiple input partitions.\n",
    "        - These transformations require data to be shuffled across the network (expensive).\n",
    "        - Spark needs to redistribute and group data before applying the transformation.\n",
    "        - Examples: groupBy, distinct, join\n",
    "\n",
    "## Actions\n",
    "- Actions are operations that trigger execution of the computation graph (DAG) built by transformations.\n",
    "- They either return a result to the driver or write it to external storage.\n",
    "- Actions launch a Spark job and use the defined transformations to compute the result.\n",
    "- Examples: show(), count(), collect(), write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8af322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|ry_user_id|age|\n",
      "+----------+---+\n",
      "|73006237  |35 |\n",
      "|74522222  |33 |\n",
      "|48241626  |34 |\n",
      "|38520311  |28 |\n",
      "|74805610  |25 |\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ry_user_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformation and action example:\n",
    "\n",
    "# Read the data \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"Read Data\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Read the data\n",
    "    spark_df = spark.read.csv(\n",
    "        r\"C:\\Users\\shubh\\OneDrive\\Desktop\\validating data.csv\", \n",
    "        header=True, #check the first row to infer column row\n",
    "        inferSchema=True # make intelligent guess of the data type of each column\n",
    "        )\n",
    "    \n",
    "    # we can chain multiple transformations together and assign the result to a new variable\n",
    "    transformed_df = spark_df.where('age between 20 and 40')\\\n",
    "                            .select('ry_user_id','age')\n",
    "\n",
    "    transformed_df.show(5, False)\n",
    "    transformed_df.printSchema()\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcef39",
   "metadata": {},
   "source": [
    "# Spark transformation and execution model\n",
    "\n",
    "## Spark Transformation Logic\n",
    "- In Spark, transformations form a Directed Acyclic Graph (DAG) of operations:\n",
    "- Example: read >> where >> select >> group >> count >> show\n",
    "- Unlike traditional programming, Spark doesn't execute operations line by line.\n",
    "- Instead, all transformations are sent to the driver, which plans the optimal execution path and sends the instructions to executors.\n",
    "- Since transformations don’t run immediately, this is known as lazy evaluation — they’re only triggered by an action.\n",
    "\n",
    "## Spark Execution Theory\n",
    "- Spark behaves like a compiler: it takes the logical plan (from your code) and compiles it into a physical execution plan.\n",
    "- When an action is called, Spark creates a job.\n",
    "- For example, reading a CSV (with schema inference) is an action and results in a job.\n",
    "- Each job is divided into stages, and each stage is broken down into tasks.\n",
    "- Every action triggers at least one job, which may contain atleast stages and tasks.\n",
    "- Spark builds a DAG of stages for each action to determine how to process the data.\n",
    "- Then spark will execute the DAG with jobs, stages and tasks etc\n",
    "\n",
    "#### Summary:\n",
    "Each action will result in job each white transformation will result in separate stage and exvery stage executes tasks in paralled depending on the number of partitions and executor cores. If there are less executors then partitions then tasks are queued.\n",
    "\n",
    "## Execution Plan Example\n",
    "\n",
    "    spark_df = spark.read.csv(\n",
    "        r\"C:\\Users\\shubh\\OneDrive\\Desktop\\validating data.csv\", \n",
    "        header=True,         # Use first row as header\n",
    "        inferSchema=True     # Infer column types automatically\n",
    "    )\n",
    "    Action 1:\n",
    "        - Triggers Job 0: Read the CSV file from disk.\n",
    "        - Triggers Job 1: Infer schema from the data.\n",
    "    transformed_df = spark_df.where('age between 20 and 40').select('ry_user_id', 'age')\n",
    "    transformed_df.show(5, False)\n",
    "    Action 2:\n",
    "        - Triggers Job 2: Apply filter and select transformations and Execute .show() to return 5 records to the driver. Since all the transforamtions are executed with only 1 action  (.show()) it will create 1 job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19f3b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|ry_user_id|age|\n",
      "+----------+---+\n",
      "|73006237  |35 |\n",
      "|74522222  |33 |\n",
      "|48241626  |34 |\n",
      "|38520311  |28 |\n",
      "|74805610  |25 |\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "press enter to continue\n"
     ]
    }
   ],
   "source": [
    "# exectuion plan example\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Read Data\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()    \n",
    "spark_df = spark.read.csv(\n",
    "        r\"C:\\Users\\shubh\\OneDrive\\Desktop\\validating data.csv\", \n",
    "        header=True, #check the first row to infer column row\n",
    "        inferSchema=True\n",
    "        )     \n",
    "transformed_df = spark_df.where('age between 20 and 40').select('ry_user_id','age')\n",
    "transformed_df.show(5, False)\n",
    "print('press enter to continue') # holding the program until user presses enter to check the spark dag\n",
    "# check the dag at localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c964f4f",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
