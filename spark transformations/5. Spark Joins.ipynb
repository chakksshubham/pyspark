{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dacc540",
   "metadata": {},
   "source": [
    "%md\n",
    "# Joins \n",
    "- joins are used while working with 2 dataframes\n",
    "- we use this to join left dataframe with right dataframe\n",
    "- To do so we need to specify tables name, join expression and type of join\n",
    "- example: df_1.join(df_2,df_1.col1 == df_2.col2,'inner')\n",
    "- inner join is the default way of join\n",
    "\n",
    "## Join name ambiguity:\n",
    "- When we join the tables, spark and sql engines in nehind the scene allots a id to the columns \n",
    "- When we have 2 tables with same columns, the name might be same but the underneath ids associated with them are different\n",
    "- So when we do \"*\" it fetches all the columns\n",
    "- But when we select the common columnn name ex- col1, it has 2 ids associted with it one for table1.col1 and another table2.col1 so we need to specify the table as well.\n",
    "\n",
    "### Approaches:\n",
    "- Rename the column before join a withColumnRenamed transformation\n",
    "- Drop the ambiguous column\n",
    "----\n",
    "## Join internals types\n",
    "  - Shuffle Join\n",
    "  - Broadcast Join\n",
    "\n",
    "## Shuffle Sort and Merge Join:\n",
    "- Most common join type in Spark.\n",
    "- Based on the Hadoop MapReduce framework.\n",
    "- Always used for large-to-large dataset joins..\n",
    "\n",
    "## Understanding the Shuffle Process\n",
    "\n",
    "### 1. Initial Setup\n",
    "For a distributed join between two datasets (`d1` and `d2`):\n",
    "- Both datasets are partitioned (example: 3 partitions each).\n",
    "  \n",
    "Example Partitioning:\n",
    "```\n",
    "d1:\n",
    "  - part-001\n",
    "  - part-002\n",
    "  - part-003\n",
    "\n",
    "d2:\n",
    "  - part-001\n",
    "  - part-002\n",
    "  - part-003\n",
    "```\n",
    "\n",
    "- These partitions are distributed across **executors**.\n",
    "  - Example: 3 executors, each holding one partition of `d1` and one partition of `d2`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Problem in Direct Joins\n",
    "- While joining on a key, the corresponding rows might **not reside** on the **same executor**.\n",
    "- Example:\n",
    "  - The key `001` from `d1` might be in `part-001` (Executor 1).\n",
    "  - The corresponding key in `d2` might be in `part-001` (Executor 2).\n",
    "- **Direct join is not possible** unless matching keys are on the same executor.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Map Phase\n",
    "- **Map phase** starts by **mapping all records based on the join key**.\n",
    "- All records are sent to a structure called the **Map Exchange** (a buffer).\n",
    "- Each record is **tagged** by its join key so Spark can track where it needs to go.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Shuffle and Reduce Phase\n",
    "- After mapping, **Spark shuffles** the data:\n",
    "  - Records are **moved across the cluster** so that **matching keys are grouped together**.\n",
    "- The **Reduce phase** reorganizes the mapped records into new **shuffle partitions**:\n",
    "  - All records with the same join key now sit together.\n",
    "  - This enables an **efficient join**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "| Term | Meaning |\n",
    "|:-----|:--------|\n",
    "| **Map Exchange** | Buffer where mapped records are stored before being shuffled. |\n",
    "| **Shuffle Operation** | Movement of records across the cluster based on join keys. |\n",
    "| **Shuffle Partitions** | Reorganized partitions after shuffle, ready for join operation. |\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Shuffle Expensive?\n",
    "- **Heavy data movement** across the network.\n",
    "- **Serialization/Deserialization** overhead.\n",
    "- **Disk I/O** if shuffle spills to disk.\n",
    "- **Memory pressure** if too much data gets shuffled at once.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Summary\n",
    "- **Shuffle Sort and Merge** is the default method for large-to-large joins.\n",
    "- It is necessary because **distributed datasets may have matching keys on different executors**.\n",
    "- The **map phase** prepares data based on the key, and the **reduce phase** rearranges it into **shuffle partitions** for joining.\n",
    "- The **shuffle** itself is the main bottleneck for join performance in Spark.\n",
    "\n",
    "## Broadcast join:\n",
    "-  A broadcast join happens when Spark copies (broadcasts) the smaller table (even upto 1-2 gb) to every node in the cluster.\n",
    "- The larger table stays partitioned. Each node joins its partition of the large table with the full copy of the small table locally.\n",
    "- When Spark automatically detects it or when we force it using broadcast().\n",
    "\n",
    "## Optimizing Joins:\n",
    "### Optimizing Shuffle Joins\n",
    "- Filtering the data before join will cut short the dataframe from join.\n",
    "- Look for times where we can aggregate the data even before joinin the dataframes to reduce the size of dataframe\n",
    "- Optimize on shuffle partitions and number of executors and unique join keys\n",
    "  - What is the maximum possible parallelism: answer is the number of executors and number of unique keys\n",
    "  - IF the dataset has 200 unique keys then each unique key can be part of 1 reduce exchange (shuffle partition) thus needing 200 shuffle partitions and each shuffle partition works on a single executor so we would need a 200 node spark cluster for running everything on parallel.\n",
    "  - Even if we have higher number of executor still the max processing for this example is limited to 200 executors.\n",
    "  - So for large cluster we should try to get the optimized shuffle partitions. if the join is limiting by unique ids then we should to increse the join cardinality.  \n",
    "  - Join cardinality = How big the output becomes after two tables are joined. Higher cardinality → more rows → more shuffle → better use of large cluster.\n",
    "- For 2 datasets example sales and product joined on key there can be key skewness. For faster moving products they all will be clustred on a shuffle partitions wheres are executors who are working with keys that correspond to slow moving products will have lesser load in their shuffle partitions. So for this we need to check tasks and break the larger shuffle partitions into smaller ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    " \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "                    .appName('Spark Joins')\n",
    "                    .enableHiveSupport()\n",
    "                    .getOrCreate()\n",
    "    )\n",
    "\n",
    "    customer_df = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            samples.tpch.customer\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    order_df = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "        samples.tpch.orders\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    display(customer_df.head(5))\n",
    "    display(order_df.head(5))\n",
    "\n",
    "    # Join the tables and find top 5 customers by products bought\n",
    "    customer_df.join(\n",
    "    order_df,\n",
    "    customer_df.c_custkey == order_df.o_custkey,\n",
    "    'left'\n",
    "    ).groupBy(\n",
    "        'c_name'\n",
    "    ).agg(\n",
    "        count('o_orderkey').alias('orderCounts')\n",
    "    ).withColumnRenamed(\n",
    "        'c_name',\n",
    "        'customerName'\n",
    "    ).orderBy(\n",
    "        col('orderCounts').desc()\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2823e25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
