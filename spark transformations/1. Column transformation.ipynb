{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35bdbc1c",
   "metadata": {},
   "source": [
    "%md \n",
    "# Defining transformations:\n",
    "- creating a new dataset with \n",
    "    - combining 2 or more dataframes\n",
    "    - aggregating and summarizing data\n",
    "    - applying various functions\n",
    "\n",
    "- Two forms of transformations:\n",
    "    - rows\n",
    "    - columns \n",
    "\n",
    "- Columns are spark object of type column\n",
    "- Column cannot be referenced outside the dataframe and manipulate indipendently \n",
    "- Columns are always used within transformation\n",
    "\n",
    "## Accessing columns \n",
    "- 2 ways to refer to a column in a dataframe\n",
    "  - col string \n",
    "    - simplest method to access col (df.select()).\n",
    "    -  spark gives bunch of transformations that take column string as transformations like select, drop, order by, group by.\n",
    "  - col object \n",
    "    - second way is to access using column object.\n",
    "    - the simplest form is using the col or column function but there are other ways as well.\n",
    "    - we can use column string and col method in same transformation as well.\n",
    "    - most of the transforamtion will offer both the options and it depends on personal choice to choose.\n",
    "\n",
    "## Creating Column expressions\n",
    "- Column expressions are formulas or transformations applied to DataFrame columns. They help you manipulate, filter, or create new columns using existing data.\n",
    "- example: \n",
    "    - col(\"age\") + 5\n",
    "    - col(\"name\").substr(1, 3)\n",
    "- In PySpark, there are two main types of column expressions:\n",
    "    - Column String expressions/SQL Expressions\n",
    "        - These are expressions written as strings, often resembling SQL syntax.\n",
    "        - When you want to write familiar SQL-style expressions, especially involving calculations or functions in a concise way.\n",
    "        - df.selectExpr(\"fare * 0.9 as discounted_fare\").show()\n",
    "        - df.withColumn(\"discounted_fare\", expr(\"fare * 0.9\"))\n",
    "    \n",
    "    - Column Object expressions\n",
    "        - These use PySpark functions and column objects (like col(\"column_name\")). \n",
    "        - They are more flexible and more readable for complex transformations.\n",
    "        - df.withColumn(\"uppercase_name\", upper(col(\"name\")))\n",
    "        - df.withColumn(\"adjusted_fare\", (col(\"fare\") * 0.9).cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName('Spark col transformation')\n",
    "        .getOrCreate()\n",
    "    )\n",
    "             \n",
    "\n",
    "    flights_df = (\n",
    "        spark.read\n",
    "        .format('csv')\n",
    "        .option('inferSchema','true')\n",
    "        .option('header','true')\n",
    "        .option('samplingRatio','0.001')\n",
    "        .load(\n",
    "            path = 'dbfs:/databricks-datasets/airlines/part-00000', # gotta download the flights dataset to work\n",
    "            encoding = 'utf-8'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ebcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.count()\n",
    "display(flights_df.head(5))\n",
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e383172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencing columns using column strings \n",
    "display(flights_df.select('FlightNum','AirTime','ArrDelay').head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencing a column using a column object \n",
    "\n",
    "display(flights_df.select(\n",
    "    column('FlightNum'),\n",
    "    col('Airtime'),\n",
    "    'ArrDelay'\n",
    ").head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column expression using column string expression\n",
    "spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
    "\n",
    "flights_df.select(\n",
    "    'Origin',\n",
    "    'Dest',\n",
    "    'Distance',\n",
    "    expr(\"to_date(concat('Year','Month','DayofMonth'),'yyyyMMdd') as flight_date\") # we have to use expr function as the select transformation only takes col object and not expression thus we need to convert the expression to column object\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27559c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column object expressions\n",
    "\n",
    "flights_df.select(\n",
    "  'Origin',\n",
    "  'Dest',\n",
    "  'Distance',\n",
    "  to_date(\n",
    "    concat(\n",
    "      'Year',\n",
    "      'Month',\n",
    "      'DayOfMonth'\n",
    "    ),\n",
    "    'yyyyMMdd'\n",
    "  ).alias('flight_date')\n",
    ").show(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
