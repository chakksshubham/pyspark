{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dac64c2",
   "metadata": {},
   "source": [
    "# Aggreagations in Spark:\n",
    "- All agg in the spark are used using built in spark functions\n",
    "- As a rule of thumb remeber to use col() when we are doing any transformation on the col like year, concat, date, arithmetic operation etc but dont use col() when refering it to any function as string like for group by, window, partition by order by etc\n",
    "- Simple Agg:\n",
    "    - Simple agg is summarize the complete dataset\n",
    "    - returns the simgle row of data\n",
    "    - example cpunt, sum, avg, min, max \n",
    "    - general syntax format df.agg(\n",
    "        agg1,\n",
    "        agg2,\n",
    "        agg3\n",
    "    )\n",
    "- Grouping Agg\n",
    "    - this is synonym to the sql group by aggregates \n",
    "    - general syntax df.groupBy(\n",
    "        [col1,\n",
    "        col2]\n",
    "    ).agg(\n",
    "        agg1,\n",
    "        agg2\n",
    "    )\n",
    "- Windowing Agg\n",
    "    - Window aggregation means applying an aggregation function (like sum, avg, count, dense_rank, etc.) across a group of rows that are related to the current row, without collapsing them into a single row (unlike groupBy).\n",
    "    - It keeps the row-level granularity but calculates aggregated values across a defined \"window\" (partition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de616a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName('Spark aggregations')\n",
    "        .master('local[*]')\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    spark_df = (\n",
    "        spark\n",
    "            .read\n",
    "            .format('csv')\n",
    "            .option('header','true')\n",
    "            .option('inferSchema','true')\n",
    "            .load(\n",
    "                path = r'C:\\Users\\shubh\\OneDrive\\Documents\\Visual Studio 2017\\datasets\\tips.csv',\n",
    "                encoder = 'utf-8'\n",
    "            )\n",
    "    )\n",
    "\n",
    "    spark_df.show(5)\n",
    "    spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ef3f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+\n",
      "|recordCount|totalBills|totalTips|avgTips|\n",
      "+-----------+----------+---------+-------+\n",
      "|        244|    4828.0|    732.0|    3.0|\n",
      "+-----------+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Aggregation: \n",
    "\n",
    "spark_df.select(\n",
    "    count('*').alias('recordCount'),\n",
    "    round(sum(col('total_bill')),0).alias('totalBills'),\n",
    "    round(sum(col('tip')),0).alias('totalTips'),\n",
    "    round(avg(col('tip')),0).alias('avgTips')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7112175",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[MISSING_GROUP_BY] The query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.;\nAggregate [total_bill#17, tip#18, sex#19, smoker#20, day#21, time#22, size#23, count(total_bill#17) AS recordCount#281L]\n+- Relation [total_bill#17,tip#18,sex#19,smoker#20,day#21,time#22,size#23] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecordCount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_bill\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m.withColumn(\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotalBillAmount\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;28msum\u001b[39m(col(\u001b[33m'\u001b[39m\u001b[33mtotal_bill\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      5\u001b[39m ).withColumn(\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotalTips\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;28msum\u001b[39m(col(\u001b[33m'\u001b[39m\u001b[33mtip\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      7\u001b[39m ).show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark-3.5.5\\python\\pyspark\\sql\\dataframe.py:5176\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   5171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   5172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   5173\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5174\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   5175\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark-3.5.5\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark-3.5.5\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [MISSING_GROUP_BY] The query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.;\nAggregate [total_bill#17, tip#18, sex#19, smoker#20, day#21, time#22, size#23, count(total_bill#17) AS recordCount#281L]\n+- Relation [total_bill#17,tip#18,sex#19,smoker#20,day#21,time#22,size#23] csv\n"
     ]
    }
   ],
   "source": [
    "spark_df.withColumn(\n",
    "    'recordCount',count(col('total_bill'))\n",
    ").withColumn(\n",
    "    'totalBillAmount',sum(col('total_bill'))\n",
    ").withColumn(\n",
    "    'totalTips',sum(col('tip'))\n",
    ").show()\n",
    "\n",
    "# we cannot do agg using withColumn because it returns a new column in the data frame with transformations but since aggregation are scaler this doens make sense\n",
    "\n",
    "# You can use withColumn() for:\n",
    "# Row-level transformations, i.e., logic applied to each row:\n",
    "\n",
    "# when/otherwise (like SQL CASE WHEN)\n",
    "\n",
    "# to_date, date_add, datediff, etc.\n",
    "\n",
    "# concat, substring, regexp_replace, etc.\n",
    "\n",
    "# udf-based transformations\n",
    "\n",
    "# Arithmetic or logic using col() expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac2e0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---------+\n",
      "|recordCount|  totalBillAmount|totalTips|\n",
      "+-----------+-----------------+---------+\n",
      "|        244|4827.770000000001|   731.58|\n",
      "+-----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.agg(\n",
    "   count(col('total_bill')).alias('recordCount'),\n",
    "    sum(col('total_bill')).alias('totalBillAmount'),\n",
    "    sum(col('tip')).alias('totalTips')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a9a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+----------+\n",
      "|  time|   sex|averageTip|averageBill|tipPercent|\n",
      "+------+------+----------+-----------+----------+\n",
      "|Dinner|  Male|      3.14|      21.46|     14.63|\n",
      "|Dinner|Female|       3.0|      19.21|     15.62|\n",
      "| Lunch|  Male|      2.88|      18.05|     15.96|\n",
      "| Lunch|Female|      2.58|      16.34|     15.79|\n",
      "+------+------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Grouped Aggregation:\n",
    "# Returns better results as per slicig and dicing requirements\n",
    "\n",
    "# use case lets find avg tips and avg bill by time and sex\n",
    "# after group by we cannot use select as its a dataframe method and after grouping it becomes grouped dataframe object\n",
    "\n",
    "\n",
    "# spark_df.groupBy(['time','sex']).select(\n",
    "#     'time',\n",
    "#     'sex',\n",
    "#     round(avg(col('tip'))).alias('averageTip'),\n",
    "#     round(avg(col('total_bill'))).alias('averageBill')\n",
    "# ).show()\n",
    "\n",
    "spark_df.groupBy(['time', 'sex']).agg(\n",
    "    round(avg(col('tip')), 2).alias('averageTip'),\n",
    "    round(avg(col('total_bill')), 2).alias('averageBill'),\n",
    "    round(((col('averageTip')/col('averageBill'))*100),2).alias('tipPercent')\n",
    ").orderBy(col('averageBill').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "498a3aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|              Region|Country|      Item Type|Sales Channel|Order Priority|Order Date| Order ID| Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|\n",
      "+--------------------+-------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|Australia and Oce...|  Palau|Office Supplies|       Online|             H|2016-03-06|517073523|2016-03-26|      2401|    651.21|   524.96|   1563555.21| 1260429.0|   303126.25|\n",
      "|              Europe| Poland|      Beverages|       Online|             L|2010-04-18|380507028|2010-05-26|      9340|     47.45|    31.79|     443183.0|  296918.6|    146264.4|\n",
      "|       North America| Canada|         Cereal|       Online|             M|2015-01-08|504055583|2015-01-31|       103|     205.7|   117.11|      21187.1|  12062.33|     9124.77|\n",
      "|              Europe|Belarus|         Snacks|       Online|             C|2014-01-19|954955518|2014-02-27|      1414|    152.58|    97.44|    215748.12| 137780.16|    77967.96|\n",
      "|Middle East and N...|   Oman|         Cereal|      Offline|             H|2019-04-26|970755660|2019-06-02|      7027|     205.7|   117.11|    1445453.9|  822932.0|   622521.94|\n",
      "+--------------------+-------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item Type: string (nullable = true)\n",
      " |-- Sales Channel: string (nullable = true)\n",
      " |-- Order Priority: string (nullable = true)\n",
      " |-- Order Date: date (nullable = true)\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Ship Date: date (nullable = true)\n",
      " |-- Units Sold: integer (nullable = true)\n",
      " |-- Unit Price: float (nullable = true)\n",
      " |-- Unit Cost: float (nullable = true)\n",
      " |-- Total Revenue: double (nullable = true)\n",
      " |-- Total Cost: float (nullable = true)\n",
      " |-- Total Profit: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema = StructType([\n",
    "    StructField ('Region',StringType()),\n",
    "    StructField ('Country',StringType()),\n",
    "    StructField ('Item Type',StringType()),\n",
    "    StructField ('Sales Channel',StringType()),\n",
    "    StructField ('Order Priority',StringType()),\n",
    "    StructField ('Order Date',StringType()),\n",
    "    StructField ('Order ID',IntegerType()),\n",
    "    StructField ('Ship Date',StringType()),\n",
    "    StructField ('Units Sold',IntegerType()),\n",
    "    StructField ('Unit Price',FloatType()),\n",
    "    StructField ('Unit Cost',FloatType()),\n",
    "    StructField ('Total Revenue',DoubleType()),\n",
    "    StructField ('Total Cost',FloatType()),\n",
    "    StructField ('Total Profit',FloatType())\n",
    "])\n",
    "\n",
    "sales_df = (\n",
    "    spark.read\n",
    "        .format('csv')\n",
    "        .option('inferSchema','true')\n",
    "        .option('header','true')\n",
    "        .load(\n",
    "            path = r'C:\\Users\\shubh\\OneDrive\\Documents\\Visual Studio 2017\\datasets\\5M Sale Transaction.csv',\n",
    "            encoder = 'utf-8',\n",
    "            schema = schema\n",
    "        )\n",
    ")\n",
    "\n",
    "spark.sql(\"SET spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
    "# gotta transform the data from string to date using function\n",
    "sales_df_transformed = sales_df.withColumn(\n",
    "    'Order Date',to_date('Order Date','MM/dd/yyyy')\n",
    ").withColumn(\n",
    "    'Ship Date',to_date('Ship Date','MM/dd/yyyy')\n",
    ")\n",
    "\n",
    "sales_df_transformed.show(5)\n",
    "sales_df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1689ade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+-------------+--------------+\n",
      "|   Country|weekOfOrder|numOrders|totalQuantity|  totalRevenue|\n",
      "+----------+-----------+---------+-------------+--------------+\n",
      "|Montenegro|         20|      545|      2895253|9.8321366431E8|\n",
      "|     Gabon|         16|      558|      2849656|9.5522715972E8|\n",
      "|  Malaysia|         23|      536|      2775604|9.5298383403E8|\n",
      "|   Ireland|         24|      537|      2718680|9.5014640829E8|\n",
      "|   Ireland|         32|      536|      2908891|9.4802659849E8|\n",
      "+----------+-----------+---------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exaple to create a new field and then perform agg with grouping on that field\n",
    "sales_df_transformed.withColumn(\n",
    "    'weekOfOrder',weekofyear(col('Order Date'))\n",
    ").groupBy(\n",
    "    [col('Country'),col('weekOfOrder')]\n",
    ").agg(\n",
    "    count(col('Order ID')).alias('numOrders'),\n",
    "    sum(col('Units Sold')).alias('totalQuantity'),\n",
    "    round(sum(col('Total revenue')),2).alias('totalRevenue')\n",
    ").orderBy(col('totalRevenue').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08595181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed Aggregations\n",
    "# example to find week on week running total using sql first\n",
    "spark.sql(\"DROP VIEW IF EXISTS global_temp.sales_view\")\n",
    "sales_df_transformed.createGlobalTempView('sales_view')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f518051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------+-------------+--------------------+--------------------+\n",
      "|    Country|week|numInvoices|totalQuantity|          totalValue|        runningTotal|\n",
      "+-----------+----+-----------+-------------+--------------------+--------------------+\n",
      "|Afghanistan|   9|         44|       229879|       8.764520492E7|       8.764520492E7|\n",
      "|Afghanistan|  10|         52|       270470| 7.351244774000001E7|1.6115765266000003E8|\n",
      "|Afghanistan|  11|         50|       288696| 8.902117754000002E7|2.5017883020000005E8|\n",
      "|Afghanistan|  12|         52|       288001|       5.803784811E7|3.0821667831000006E8|\n",
      "|Afghanistan|  13|         18|       107396|3.1638664630000003E7|3.3985534294000006E8|\n",
      "|    Albania|   9|         41|       214742|6.1229220059999995E7|6.1229220059999995E7|\n",
      "|    Albania|  10|         42|       270596| 5.151935969999999E7|1.1274857975999999E8|\n",
      "|    Albania|  11|         53|       269077| 6.478094346000001E7|      1.7752952322E8|\n",
      "|    Albania|  12|         47|       249395|       5.444069947E7|      2.3197022269E8|\n",
      "|    Albania|  13|         21|        92941|       2.572771128E7|      2.5769793397E8|\n",
      "|    Algeria|   9|         42|       173358|       3.566383594E7|       3.566383594E7|\n",
      "|    Algeria|  10|         47|       194244| 5.420026103000001E7|       8.986409697E7|\n",
      "|    Algeria|  11|         49|       238431|4.0464368349999994E7|      1.3032846532E8|\n",
      "|    Algeria|  12|         52|       294106|       8.660441384E7|      2.1693287916E8|\n",
      "|    Algeria|  13|         22|       129822|       5.270157008E7|      2.6963444924E8|\n",
      "|    Andorra|   9|         54|       230070|       6.041962543E7|       6.041962543E7|\n",
      "|    Andorra|  10|         50|       210648| 6.052708668000001E7|1.2094671211000001E8|\n",
      "|    Andorra|  11|         41|       203687|       4.796848105E7|1.6891519316000003E8|\n",
      "|    Andorra|  12|         44|       261814|1.0546077722999997E8|      2.7437597039E8|\n",
      "|    Andorra|  13|         16|        91630|4.7677879720000006E7|      3.2205385011E8|\n",
      "+-----------+----+-----------+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying first with spark sql \n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH cte as (\n",
    "    SELECT\n",
    "        Country,\n",
    "        weekofyear(CAST(`Order Date` AS DATE)) AS week,\n",
    "        COUNT(`Order ID`) AS numInvoices,\n",
    "        SUM(`Units Sold`) AS totalQuantity,\n",
    "        SUM(`Total Revenue`) AS totalValue    \n",
    "    FROM \n",
    "        sales_view\n",
    "    WHERE \n",
    "        Year(`Order Date`) = 2010\n",
    "        AND MONth(`Order Date`) = 03\n",
    "    GROUP BY\n",
    "        Country,\n",
    "        weekofyear(CAST(`Order Date` AS DATE))\n",
    "    Order By Country, week)\n",
    "    Select\n",
    "        *,\n",
    "        sum(totalValue) over (\n",
    "            partition by Country order by week asc\n",
    "        ) as runningTotal\n",
    "    From cte\n",
    "    \"\"\"\n",
    ").show()\n",
    "# we have to use `` as '' this is considered string and function fails also if there are no space we can use without any quotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81ec77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-------------+--------------+--------------+\n",
      "|    Country|weekOfOrder|numOrders|totalQuantity|  totalRevenue|  runningTotal|\n",
      "+-----------+-----------+---------+-------------+--------------+--------------+\n",
      "|Afghanistan|          9|       44|       229879| 8.764520492E7| 8.764520492E7|\n",
      "|Afghanistan|         10|       52|       270470| 7.351244774E7|1.6115765266E8|\n",
      "|Afghanistan|         11|       50|       288696| 8.902117754E7| 2.501788302E8|\n",
      "|Afghanistan|         12|       52|       288001| 5.803784811E7|3.0821667831E8|\n",
      "|Afghanistan|         13|       18|       107396| 3.163866463E7|3.3985534294E8|\n",
      "|    Albania|          9|       41|       214742| 6.122922006E7| 6.122922006E7|\n",
      "|    Albania|         10|       42|       270596|  5.15193597E7|1.1274857976E8|\n",
      "|    Albania|         11|       53|       269077| 6.478094346E7|1.7752952322E8|\n",
      "|    Albania|         12|       47|       249395| 5.444069947E7|2.3197022269E8|\n",
      "|    Albania|         13|       21|        92941| 2.572771128E7|2.5769793397E8|\n",
      "|    Algeria|          9|       42|       173358| 3.566383594E7| 3.566383594E7|\n",
      "|    Algeria|         10|       47|       194244| 5.420026103E7| 8.986409697E7|\n",
      "|    Algeria|         11|       49|       238431| 4.046436835E7|1.3032846532E8|\n",
      "|    Algeria|         12|       52|       294106| 8.660441384E7|2.1693287916E8|\n",
      "|    Algeria|         13|       22|       129822| 5.270157008E7|2.6963444924E8|\n",
      "|    Andorra|          9|       54|       230070| 6.041962543E7| 6.041962543E7|\n",
      "|    Andorra|         10|       50|       210648| 6.052708668E7|1.2094671211E8|\n",
      "|    Andorra|         11|       41|       203687| 4.796848105E7|1.6891519316E8|\n",
      "|    Andorra|         12|       44|       261814|1.0546077723E8|2.7437597039E8|\n",
      "|    Andorra|         13|       16|        91630| 4.767787972E7|3.2205385011E8|\n",
      "+-----------+-----------+---------+-------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sales_df_transformed.filter(\n",
    "    (year('Order Date') == 2010) & (month('Order Date') == 3)\n",
    ").withColumn(\n",
    "    'weekOfOrder',weekofyear(col('Order Date'))\n",
    ").groupBy(\n",
    "    ['Country','weekOfOrder']\n",
    ").agg(\n",
    "    count(col('Order ID')).alias('numOrders'),\n",
    "    sum(col('Units Sold')).alias('totalQuantity'),\n",
    "    round(sum(col('Total Revenue')),2).alias('totalRevenue')\n",
    ").orderBy(\n",
    "    col('totalRevenue').desc()\n",
    ").withColumn(\n",
    "    'runningTotal',\n",
    "    sum(col('totalRevenue')).over(\n",
    "        Window.partitionBy(\n",
    "            'Country'\n",
    "        ).orderBy(\n",
    "            'weekOfOrder'\n",
    "        )\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb37912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example to find top 5 country by region for total_revenue and if same rank by profit margin for the year 2016 march month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2064f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|              Region|Country|      Item Type|Sales Channel|Order Priority|Order Date| Order ID| Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|\n",
      "+--------------------+-------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "|Australia and Oce...|  Palau|Office Supplies|       Online|             H|2016-03-06|517073523|2016-03-26|      2401|    651.21|   524.96|   1563555.21| 1260429.0|   303126.25|\n",
      "|              Europe| Poland|      Beverages|       Online|             L|2010-04-18|380507028|2010-05-26|      9340|     47.45|    31.79|     443183.0|  296918.6|    146264.4|\n",
      "|       North America| Canada|         Cereal|       Online|             M|2015-01-08|504055583|2015-01-31|       103|     205.7|   117.11|      21187.1|  12062.33|     9124.77|\n",
      "|              Europe|Belarus|         Snacks|       Online|             C|2014-01-19|954955518|2014-02-27|      1414|    152.58|    97.44|    215748.12| 137780.16|    77967.96|\n",
      "|Middle East and N...|   Oman|         Cereal|      Offline|             H|2019-04-26|970755660|2019-06-02|      7027|     205.7|   117.11|    1445453.9|  822932.0|   622521.94|\n",
      "+--------------------+-------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "309f29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+----+\n",
      "|              Region|             Country|        totalRevenue|totalProfitMargin|Rank|\n",
      "+--------------------+--------------------+--------------------+-----------------+----+\n",
      "|                Asia|              Bhutan|4.1906714101000005E8|75.61299055704819|   1|\n",
      "|                Asia|            Mongolia|      3.6116925598E8|67.67414230840438|   2|\n",
      "|                Asia|           Singapore|3.5950970765999997E8| 73.4750026546758|   3|\n",
      "|Australia and Oce...|               Tonga|3.4420216186999995E8|67.58031595391664|   1|\n",
      "|Australia and Oce...|                Fiji|3.4196462662999994E8|72.02509018983733|   2|\n",
      "|Australia and Oce...|         New Zealand|      3.4134434464E8|78.38372755054573|   3|\n",
      "|Central America a...|             Grenada|3.2272678528999996E8|64.79109626833556|   1|\n",
      "|Central America a...|                Cuba|      3.1608106588E8|74.58269993889917|   2|\n",
      "|Central America a...|           Guatemala|      3.1563886009E8|78.28083915851667|   3|\n",
      "|              Europe|            Moldova |       3.819153486E8|70.72374211675886|   1|\n",
      "|              Europe|             Denmark|      3.7883061336E8|68.39923543616601|   2|\n",
      "|              Europe|       Liechtenstein|3.6284598438000005E8|80.58358255597719|   3|\n",
      "|Middle East and N...|                Oman|      3.5961199201E8|76.49907977230902|   1|\n",
      "|Middle East and N...|              Turkey|       3.571732748E8|70.22846422276304|   2|\n",
      "|Middle East and N...|                Iraq|      3.4262529074E8|65.82215550245172|   3|\n",
      "|       North America|United States of ...|3.1338260479999995E8|75.69499516596073|   1|\n",
      "|       North America|              Mexico|      2.6917197001E8|67.64851949739435|   2|\n",
      "|       North America|           Greenland|2.5620391453000003E8|75.50985413917144|   3|\n",
      "|  Sub-Saharan Africa|          The Gambia|      4.1275756721E8|76.36941708749093|   1|\n",
      "|  Sub-Saharan Africa|               Niger| 3.472909191999999E8|71.62990487769584|   2|\n",
      "+--------------------+--------------------+--------------------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df_transformed.filter(\n",
    "    (year(col('Order Date')) == 2016) & (month(col('Order Date')) == 3)\n",
    ").withColumn(\n",
    "    'profitMargin',\n",
    "    (col('Total Profit')/col('Total Revenue'))\n",
    ").groupBy(\n",
    "    [\n",
    "        'Region',\n",
    "        'Country'\n",
    "    ]\n",
    ").agg(\n",
    "    sum(col('Total Revenue')).alias('totalRevenue'),\n",
    "    sum(col('profitMargin')).alias('totalProfitMargin')\n",
    ").withColumn(\n",
    "    'Rank',\n",
    "    dense_rank().over(\n",
    "        Window.partitionBy(\n",
    "            'Region'\n",
    "        ).orderBy(\n",
    "           [\n",
    "               col('totalRevenue').desc(),\n",
    "               col('totalProfitMargin').desc()\n",
    "           ] \n",
    "        )\n",
    "    )\n",
    ").filter(\n",
    "    col('Rank') <= 3\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25cc70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
